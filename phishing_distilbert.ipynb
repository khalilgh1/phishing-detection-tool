{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2504297",
   "metadata": {},
   "source": [
    "# Phishing Email Detection with DistilBERT\n",
    "\n",
    "Binary text classification: **phishing (1)** vs **legitimate (0)** emails.\n",
    "\n",
    "- Model: [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased)\n",
    "- Framework: HuggingFace Transformers + PyTorch\n",
    "- Runtime: Google Colab (GPU recommended)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c76b9b",
   "metadata": {},
   "source": [
    "## 1 â€” Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4159567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/5d/e6/ec8471c8072382cb91233ba7267fd931219753bb43814cbc71757bfd4dab/safetensors-0.7.0-cp38-abi3-win_amd64.whl\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E38984F800>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/5d/e6/ec8471c8072382cb91233ba7267fd931219753bb43814cbc71757bfd4dab/safetensors-0.7.0-cp38-abi3-win_amd64.whl\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E389DBD040>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/5d/e6/ec8471c8072382cb91233ba7267fd931219753bb43814cbc71757bfd4dab/safetensors-0.7.0-cp38-abi3-win_amd64.whl\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E389C93290>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/5d/e6/ec8471c8072382cb91233ba7267fd931219753bb43814cbc71757bfd4dab/safetensors-0.7.0-cp38-abi3-win_amd64.whl\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E389C930E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/5d/e6/ec8471c8072382cb91233ba7267fd931219753bb43814cbc71757bfd4dab/safetensors-0.7.0-cp38-abi3-win_amd64.whl\n",
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets accelerate scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537f0d4",
   "metadata": {},
   "source": [
    "## 2 â€” Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc40880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.7' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:56: UserWarning: Pandas requires version '1.4.2' or newer of 'bottleneck' (version '1.3.7' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a37dd",
   "metadata": {},
   "source": [
    "## 3 â€” Load Dataset\n",
    "\n",
    "Upload your CSV file to Colab (or mount Google Drive).  \n",
    "The CSV must have at least two columns:\n",
    "- **`text`** â€” the email content (string)\n",
    "- **`label`** â€” binary target (`1` = phishing, `0` = legitimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a21b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# â”€â”€ Option A: Upload CSV directly in Colab â”€â”€\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# from google.colab import files\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# uploaded = files.upload()  # opens a file picker\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# CSV_PATH = list(uploaded.keys())[0]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# â”€â”€ Option B: Set path manually â”€â”€\u001b[39;00m\n\u001b[0;32m      7\u001b[0m CSV_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# <-- change this to your file path\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    861\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    862\u001b[0m     dialect,\n\u001b[0;32m    863\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    869\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    870\u001b[0m )\n\u001b[0;32m    871\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    297\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1903\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1915\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    935\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Option A: Upload CSV directly in Colab â”€â”€\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # opens a file picker\n",
    "# CSV_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# â”€â”€ Option B: Set path manually â”€â”€\n",
    "CSV_PATH = \"dataset.csv\"  # <-- change this to your file path\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95b04f",
   "metadata": {},
   "source": [
    "## 4 â€” Explore & Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Rename columns if needed â”€â”€\n",
    "# Uncomment and adjust if your columns are named differently:\n",
    "# df = df.rename(columns={\"email_body\": \"text\", \"is_phishing\": \"label\"})\n",
    "\n",
    "# Ensure required columns exist\n",
    "assert \"text\" in df.columns, \"CSV must have a 'text' column\"\n",
    "assert \"label\" in df.columns, \"CSV must have a 'label' column\"\n",
    "\n",
    "# Drop rows with missing text\n",
    "df = df.dropna(subset=[\"text\"])\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"\\nPhishing ratio: {df['label'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution bar chart\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "df[\"label\"].value_counts().plot(kind=\"bar\", color=[\"#2ecc71\", \"#e74c3c\"], ax=ax)\n",
    "ax.set_xticklabels([\"Legitimate (0)\", \"Phishing (1)\"], rotation=0)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Label Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a40b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "df[\"text_len\"] = df[\"text\"].str.len()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "df.groupby(\"label\")[\"text_len\"].plot(kind=\"hist\", bins=50, alpha=0.6, ax=ax,\n",
    "                                      legend=True)\n",
    "ax.set_xlabel(\"Character length\")\n",
    "ax.set_title(\"Text Length Distribution by Label\")\n",
    "ax.legend([\"Legitimate\", \"Phishing\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(df.groupby(\"label\")[\"text_len\"].describe().round(0))\n",
    "df = df.drop(columns=[\"text_len\"])  # cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e2079",
   "metadata": {},
   "source": [
    "## 5 â€” Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8  # 80% train, 20% validation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df[[\"text\", \"label\"]],\n",
    "    test_size=1 - TRAIN_RATIO,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Val:   {len(val_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37580d",
   "metadata": {},
   "source": [
    "## 6 â€” Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 512  # DistilBERT max context length\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize a batch of texts with padding and truncation.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert pandas DataFrames to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "print(f\"Tokenized train: {train_dataset}\")\n",
    "print(f\"Tokenized val:   {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8f458",
   "metadata": {},
   "source": [
    "## 7 â€” Load Pre-trained DistilBERT for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 2  # binary classification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194cdb3d",
   "metadata": {},
   "source": [
    "## 8 â€” Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./distilbert-phishing\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),  # mixed precision on GPU\n",
    "    report_to=\"none\",  # disable W&B / MLflow logging\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046b04f",
   "metadata": {},
   "source": [
    "## 9 â€” Metrics Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy, precision, recall, and F1 for the Trainer.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54928f51",
   "metadata": {},
   "source": [
    "## 10 â€” Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cbd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Total steps: {train_result.global_step}\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f72ab5",
   "metadata": {},
   "source": [
    "## 11 â€” Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ed8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"Validation Results\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in eval_results.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        name = key.replace(\"eval_\", \"\").capitalize()\n",
    "        print(f\"  {name:>12s}: {value:.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f7200",
   "metadata": {},
   "source": [
    "## 12 â€” Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e5985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the validation set\n",
    "preds_output = trainer.predict(val_dataset)\n",
    "y_pred = np.argmax(preds_output.predictions, axis=-1)\n",
    "y_true = val_df[\"label\"].values\n",
    "\n",
    "# Classification report\n",
    "target_names = [\"Legitimate (0)\", \"Phishing (1)\"]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=target_names, yticklabels=target_names, ax=ax)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda9c7ba",
   "metadata": {},
   "source": [
    "## 13 â€” Save Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e01cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"./distilbert-phishing-final\"\n",
    "\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {SAVE_DIR}\")\n",
    "print(f\"Contents: {os.listdir(SAVE_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225821b1",
   "metadata": {},
   "source": [
    "## 14 â€” Inference: Predict on New Emails\n",
    "\n",
    "Use the trained model to classify any email text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_email(text: str, model=model, tokenizer=tokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    Classify a single email as phishing or legitimate.\n",
    "\n",
    "    Returns:\n",
    "        dict with 'label' (0 or 1), 'label_name', and 'confidence'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0][pred_label].item()\n",
    "\n",
    "    label_name = \"Phishing\" if pred_label == 1 else \"Legitimate\"\n",
    "    return {\n",
    "        \"label\": pred_label,\n",
    "        \"label_name\": label_name,\n",
    "        \"confidence\": round(confidence, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test with sample emails â”€â”€\n",
    "\n",
    "test_emails = [\n",
    "    \"Dear user, your account has been compromised. Click here immediately to verify your identity: http://totallylegit-bank.com/login\",\n",
    "    \"Hi team, please find attached the Q3 financial report. Let me know if you have any questions. Best regards, Sarah\",\n",
    "    \"URGENT: You have won a $1,000,000 lottery! Claim your prize now by sending your bank details to claim@prize-winner.net\",\n",
    "    \"Meeting reminder: Project sync tomorrow at 10 AM in Conference Room B. Agenda attached.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Single Email Predictions\")\n",
    "print(\"=\" * 60)\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    result = predict_email(email)\n",
    "    status = \"ðŸš¨\" if result[\"label\"] == 1 else \"âœ…\"\n",
    "    print(f\"\\n{status} Email {i}:\")\n",
    "    print(f\"   Text:       {email[:80]}...\")\n",
    "    print(f\"   Prediction: {result['label_name']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dac6e",
   "metadata": {},
   "source": [
    "## 15 â€” (Optional) Download Model from Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download the saved model\n",
    "# Uncomment the lines below when running in Colab\n",
    "\n",
    "# import shutil\n",
    "# shutil.make_archive(\"distilbert-phishing-final\", \"zip\", SAVE_DIR)\n",
    "# from google.colab import files\n",
    "# files.download(\"distilbert-phishing-final.zip\")\n",
    "# print(\"Download started!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
